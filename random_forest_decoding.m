% Assuming you have already loaded and preprocessed your data
% neuron_mean_concats is a 300x200 array with 300 rows (neurons) and 200 columns (samples)
% uses neuron_num (generated by access_risk_inscopix_v2.m) to set the
% numNeuronsPerCondition parameter

neuron_mean_concat = neuron_mean_concat_DECODING;
%% Random Forest (this seemed to work best to decode Blocks of Large Rew Trials (B1 Large Choice Aligned, B2 Large Choice Aligned, B3 Large Choice Aligned)

% FROM RUAIRI - KEEP UPDATING BECAUSE HE SUGGESTS THAT THIS IS A BETTER STRUCTURE
% currently only works with 2 predictors, while testing
% the y variable here still doesn't seem right - or at least it is not used
% correctly to split the data as far as I can tell
neuron_mean_concat_PCA_redo = [neuron_mean_concat_PCA(:, 101:200) neuron_mean_concat_PCA(:, 301:400)];
figure; plot(mean(neuron_mean_concat_PCA(:, 101:200))); hold on; plot(mean(neuron_mean_concat_PCA(:, 301:400))); hold off



numConditions = iter;

y_redo =  [ones(1,100) ones(1,100)*2];

% Split data into training and testing sets
[trainIdx, testIdx] = crossvalind('HoldOut', size(neuron_mean_concat_PCA_redo, 2), 0.2);
XTrain = neuron_mean_concat_PCA_redo(trainIdx, 1:end);
yTrain = y_redo(trainIdx);
XTest = neuron_mean_concat_PCA_redo(testIdx, 1:end);
yTest = y_redo(testIdx);

% Train a Random Forest classification model
numTrees = 100; % Number of decision trees in the forest
model = TreeBagger(numTrees, XTrain, yTrain, 'Method', 'classification');

% Predict trial block labels using the trained model
predictions = predict(model, XTest);

% Convert cell array of strings to numeric labels
predictions = str2double(predictions);

% Evaluate the model's performance
correctPredictions = sum(predictions == yTest');
totalSamples = numel(yTest);
accuracy = correctPredictions / totalSamples;
confusionMat = confusionmat(yTest, predictions);

% Visualize the confusion matrix
heatmap(confusionMat, 'XLabel', 'Predicted', 'YLabel', 'Actual');

disp(['Accuracy: ' num2str(accuracy)]);


figure; 
hold on;
for ii = 1:numConditions
    plot(ts1(1, 1:100), mean(neuron_mean_concat_PCA_redo(:, y_redo==ii)));
end
hold off;

%% FROM RUAIRI - KEEP UPDATING BECAUSE THIS IS A BETTER STRUCTURE
neuron_mean_concat_PCA_redo = [neuron_mean_concat_PCA(:, 101:200) neuron_mean_concat_PCA(:, 301:400)];

% shuffle and resample the data to generate data for a circularly shuffled Random Forest decoding
trialCt = size(neuron_mean_concat_PCA_redo,2); %number of trials for currently analyzed event
for g = 1:1000 %for each resampling of the data g = 1:uv.resamples
    [num_timepoints, num_signals] = size(neuron_mean_concat_PCA_redo);
    shuffled_data = zeros(num_timepoints, num_signals); % Preallocate matrix for efficiency

    for i = 1:num_timepoints
        shift_val = randi(num_signals); % Generate a random shift value for each signal
        shuffled_data(i,:) = circshift(neuron_mean_concat_PCA_redo(i,:), shift_val,2); % Perform the circular shuffle
    end
    nullDistTrace(g,:) = nanmean(shuffled_data); %calculate the NaN mean of the shuffled traces
    %calculate the NaN mean of the shuffled event rates
end
% clear shuffled* g t trialCt



y_redo =  [ones(1,100) ones(1,100)*2];

% Split data into training and testing sets
[trainIdx, testIdx] = crossvalind('HoldOut', size(shuffled_data, 2), 0.2);
XTrain = shuffled_data(trainIdx, 1:end);
yTrain = y_redo(trainIdx);
XTest = shuffled_data(testIdx, 1:end);
yTest = y_redo(testIdx);

% Train a Random Forest classification model
numTrees = 100; % Number of decision trees in the forest
model = TreeBagger(numTrees, XTrain, yTrain, 'Method', 'classification');

% Predict trial block labels using the trained model
predictions = predict(model, XTest);

% Convert cell array of strings to numeric labels
predictions = str2double(predictions);

% Evaluate the model's performance
correctPredictions = sum(predictions == yTest');
totalSamples = numel(yTest);
accuracy = correctPredictions / totalSamples;
confusionMat = confusionmat(yTest, predictions);

% Visualize the confusion matrix
heatmap(confusionMat, 'XLabel', 'Predicted', 'YLabel', 'Actual');

disp(['Accuracy: ' num2str(accuracy)]);


figure; 
hold on;
for ii = 1:numConditions
    plot(ts1, mean(neuron_mean_concat(y==ii, 1:end)));
end
hold off;


%%
% Set the number of folds
numFolds = 5;

% Initialize variables to store overall performance metrics
overallCorrectPredictions = 0;
overallTotalSamples = 0;

% Loop over folds
for fold = 1:numFolds
    % Create indices for the current fold
        
    
    % Extract training and testing sets for the current fold
    XTrain = neuron_mean_concat_PCA_redo(cvIdx ~= fold, :);
    yTrain = y_redo(cvIdx ~= fold);
    XTest = neuron_mean_concat_PCA_redo(cvIdx == fold, :);
    yTest = y_redo(cvIdx == fold);

    % Train a Random Forest classification model
    numTrees = 100; % Number of decision trees in the forest
    model = TreeBagger(numTrees, XTrain, yTrain, 'Method', 'classification');

    % Predict trial block labels using the trained model
    predictions = predict(model, XTest);

    % Convert cell array of strings to numeric labels
    predictions = str2double(predictions);

    % Evaluate the model's performance for the current fold
    correctPredictions = sum(predictions == yTest');
    totalSamples = numel(yTest);

    % Update overall performance metrics
    overallCorrectPredictions = overallCorrectPredictions + correctPredictions;
    overallTotalSamples = overallTotalSamples + totalSamples;

    % Visualize the confusion matrix for the current fold
    confusionMat = confusionmat(yTest, predictions);
    figure;
    heatmap(confusionMat, 'XLabel', 'Predicted', 'YLabel', 'Actual');
    title(['Confusion Matrix - Fold ' num2str(fold)]);
end

% Calculate overall accuracy
overallAccuracy = overallCorrectPredictions / overallTotalSamples;

% Display overall accuracy
disp(['Overall Accuracy: ' num2str(overallAccuracy)]);

%% Random Forest (this seemed to work best to decode Blocks of Large Rew Trials (B1 Large Choice Aligned, B2 Large Choice Aligned, B3 Large Choice Aligned)
% these code work, but might not be totally correct according to Ruairi


% Create label vector y (corresponding to trial blocks)
numNeuronsPerCondition = neuron_num;
% change depending on the number of behaviors to decode!
numConditions = size(neuron_mean_concat, 1)/numNeuronsPerCondition;




y = repelem(1:numConditions, numNeuronsPerCondition);

% Split data into training and testing sets
[trainIdx, testIdx] = crossvalind('HoldOut', size(neuron_mean_concat, 1), 0.2);
XTrain = neuron_mean_concat(trainIdx, 1:end);
yTrain = y(trainIdx);
XTest = neuron_mean_concat(testIdx, 1:end);
yTest = y(testIdx);

% Train a Random Forest classification model
numTrees = 100; % Number of decision trees in the forest
model = TreeBagger(numTrees, XTrain, yTrain, 'Method', 'classification');

% Predict trial block labels using the trained model
predictions = predict(model, XTest);

% Convert cell array of strings to numeric labels
predictions = str2double(predictions);

% Evaluate the model's performance
correctPredictions = sum(predictions == yTest');
totalSamples = numel(yTest);
accuracy = correctPredictions / totalSamples;
confusionMat = confusionmat(yTest, predictions);

% Visualize the confusion matrix
heatmap(confusionMat, 'XLabel', 'Predicted', 'YLabel', 'Actual');

disp(['Accuracy: ' num2str(accuracy)]);


figure; 
hold on;
for ii = 1:numConditions
    plot(ts1, mean(neuron_mean_concat(y==ii, 1:end)));
end
hold off;

% Define the number of classes
numClasses = numConditions;

% Initialize arrays to store precision, recall, and F1-score for each class
precisionArray = zeros(1, numClasses);
recallArray = zeros(1, numClasses);
f1ScoreArray = zeros(1, numClasses);

% Initialize arrays to store TP, FP, and FN for micro-average
TP_micro = zeros(1, numClasses);
FP_micro = zeros(1, numClasses);
FN_micro = zeros(1, numClasses);

for classIdx = 1:numClasses
    % True positives, false positives, false negatives, and true negatives for the current class
    TP = sum(predictions' == classIdx & yTest == classIdx); % True Positives
    FP = sum(predictions' == classIdx & yTest ~= classIdx); % False Positives
    FN = sum(predictions' ~= classIdx & yTest == classIdx); % False Negatives
    
    % Precision, Recall, and F1-Score calculation for the current class
    precision = TP / (TP + FP);
    recall = TP / (TP + FN);
    f1Score = 2 * (precision * recall) / (precision + recall);

    % Store the metrics for the current class
    precisionArray(classIdx) = precision;
    recallArray(classIdx) = recall;
    f1ScoreArray(classIdx) = f1Score;
    
    % Store TP, FP, FN for micro-average
    TP_micro(classIdx) = TP;
    FP_micro(classIdx) = FP;
    FN_micro(classIdx) = FN;

    % Display precision, recall, and F1-score for the current class
    disp(['Class ' num2str(classIdx) ' Precision: ' num2str(precision)]);
    disp(['Class ' num2str(classIdx) ' Recall: ' num2str(recall)]);
    disp(['Class ' num2str(classIdx) ' F1-Score: ' num2str(f1Score)]);
end

% Calculate macro-average precision, recall, and F1-Score
macroPrecision = mean(precisionArray);
macroRecall = mean(recallArray);
macroF1Score = mean(f1ScoreArray);

% Calculate micro-average precision, recall, and F1-Score
microPrecision = sum(TP_micro) / (sum(TP_micro) + sum(FP_micro));
microRecall = sum(TP_micro) / (sum(TP_micro) + sum(FN_micro));
microF1Score = 2 * (microPrecision * microRecall) / (microPrecision + microRecall);

% Display macro and micro averages
disp(['Macro-Average Precision: ' num2str(macroPrecision)]);
disp(['Macro-Average Recall: ' num2str(macroRecall)]);
disp(['Macro-Average F1-Score: ' num2str(macroF1Score)]);
disp(['Micro-Average Precision: ' num2str(microPrecision)]);
disp(['Micro-Average Recall: ' num2str(microRecall)]);
disp(['Micro-Average F1-Score: ' num2str(microF1Score)]);



%% Random Forest decoder accuracy over time - this takes a while to run but seems to work. requires numTrees from previous step 
% Initialize variables to store accuracy at each time point

numTimePoints = size(XTest, 2);  % Number of time points
accuracyAtTime = zeros(1, numTimePoints);

%Train a Random Forest classification model for each time point
for t = 1:numTimePoints
    XTrainTime = XTrain(:, 1:t);  % Use data up to the current time point
    XTestTime = XTest(:, 1:t);    % Use data up to the current time point for testing

    % Train a Random Forest classification model for this time point
    modelTime = TreeBagger(numTrees, XTrainTime, yTrain, 'Method', 'classification');

    % Predict trial block labels using the trained model for this time point
    predictionsTime = predict(modelTime, XTestTime);

    % Convert cell array of strings to numeric labels
    predictionsTime = str2double(predictionsTime);

    % Calculate accuracy for this time point
    correctPredictionsTime = sum(predictionsTime == yTest');
    totalSamplesTime = numel(yTest);
    accuracyAtTime(t) = correctPredictionsTime / totalSamplesTime;
end

%Plot accuracy at each time point
figure;
plot(1:numTimePoints, accuracyAtTime);
xlabel('Time Point');
ylabel('Accuracy');
title('Accuracy at Each Time Point');

%% Random forest accuracy over time, but not cumulative. This results in much lower accuracy since it is working with only one sample at a time, and may not be that useful
% Initialize variables to store accuracy at each time point
numTimePoints = size(XTest, 2);  % Number of time points
accuracyAtTime = zeros(1, numTimePoints);

% Train a Random Forest classification model for each time point
for t = 1:numTimePoints
    XTrainTime = XTrain(:, t);  % Use data at the current time point
    XTestTime = XTest(:, t);    % Use data at the current time point for testing

    % Train a Random Forest classification model for this time point
    modelTime = TreeBagger(numTrees, XTrainTime, yTrain, 'Method', 'classification');

    % Predict trial block labels using the trained model for this time point
    predictionsTime = predict(modelTime, XTestTime);

    % Convert cell array of strings to numeric labels
    predictionsTime = str2double(predictionsTime);

    % Calculate accuracy for this time point
    correctPredictionsTime = sum(predictionsTime == yTest');
    totalSamplesTime = numel(yTest);
    accuracyAtTime(t) = correctPredictionsTime / totalSamplesTime;
end

% Plot accuracy at each time point
figure;
plot(1:numTimePoints, accuracyAtTime);
xlabel('Time Point');
ylabel('Accuracy');
title('Accuracy at Each Time Point');
%% shuffle and resample the data to generate data for a circularly shuffled Random Forest decoding
trialCt = size(neuron_mean_concat,1); %number of trials for currently analyzed event
for g = 1:1000 %for each resampling of the data g = 1:uv.resamples
    [num_timepoints, num_signals] = size(neuron_mean_concat);
    shuffled_data = zeros(num_timepoints, num_signals); % Preallocate matrix for efficiency

    for i = 1:num_timepoints
        shift_val = randi(num_signals); % Generate a random shift value for each signal
        shuffled_data(i,:) = circshift(neuron_mean_concat(i,:), shift_val,2); % Perform the circular shuffle
    end
    nullDistTrace(g,:) = nanmean(shuffled_data); %calculate the NaN mean of the shuffled traces
    %calculate the NaN mean of the shuffled event rates
end
% clear shuffled* g t trialCt

% Random Forest on shuffled data
% Assuming you have already loaded and preprocessed your data
% neuron_mean_concats is a 300x200 array with 300 rows (neurons) and 200 columns (samples)

% Create label vector y (corresponding to trial blocks)
numNeuronsPerCondition = neuron_num;
% change depending on the number of behaviors to decode!
numConditions = size(shuffled_data, 1)/numNeuronsPerCondition;




y = repelem(1:numConditions, numNeuronsPerCondition);

% Split data into training and testing sets
[trainIdx, testIdx] = crossvalind('HoldOut', size(shuffled_data, 1), 0.2);
XTrain = shuffled_data(trainIdx, 1:end-1);
yTrain = y(trainIdx);
XTest = shuffled_data(testIdx, 1:end-1);
yTest = y(testIdx);

% Train a Random Forest classification model
numTrees = 100; % Number of decision trees in the forest
model = TreeBagger(numTrees, XTrain, yTrain, 'Method', 'classification');

% Predict trial block labels using the trained model
predictions = predict(model, XTest);

% Convert cell array of strings to numeric labels
predictions = str2double(predictions);

% Evaluate the model's performance
correctPredictions = sum(predictions == yTest');
totalSamples = numel(yTest);
accuracy = correctPredictions / totalSamples;
confusionMat = confusionmat(yTest, predictions);

% Visualize the confusion matrix
heatmap(confusionMat, 'XLabel', 'Predicted', 'YLabel', 'Actual');

disp(['Accuracy: ' num2str(accuracy)]);


figure; 
hold on;
for ii = 1:numConditions
    plot(ts1, mean(shuffled_data(y==ii, 1:end)));
end
hold off;
% Initialize arrays to store precision, recall, and F1-score for each class
precisionArray = zeros(1, numClasses);
recallArray = zeros(1, numClasses);
f1ScoreArray = zeros(1, numClasses);

% Initialize arrays to store TP, FP, and FN for micro-average
TP_micro = zeros(1, numClasses);
FP_micro = zeros(1, numClasses);
FN_micro = zeros(1, numClasses);

for classIdx = 1:numClasses
    % True positives, false positives, false negatives, and true negatives for the current class
    TP = sum(predictions' == classIdx & yTest == classIdx); % True Positives
    FP = sum(predictions' == classIdx & yTest ~= classIdx); % False Positives
    FN = sum(predictions' ~= classIdx & yTest == classIdx); % False Negatives
    
    % Precision, Recall, and F1-Score calculation for the current class
    precision = TP / (TP + FP);
    recall = TP / (TP + FN);
    f1Score = 2 * (precision * recall) / (precision + recall);

    % Store the metrics for the current class
    precisionArray(classIdx) = precision;
    recallArray(classIdx) = recall;
    f1ScoreArray(classIdx) = f1Score;
    
    % Store TP, FP, FN for micro-average
    TP_micro(classIdx) = TP;
    FP_micro(classIdx) = FP;
    FN_micro(classIdx) = FN;

    % Display precision, recall, and F1-score for the current class
    disp(['Class ' num2str(classIdx) ' Precision: ' num2str(precision)]);
    disp(['Class ' num2str(classIdx) ' Recall: ' num2str(recall)]);
    disp(['Class ' num2str(classIdx) ' F1-Score: ' num2str(f1Score)]);
end

% Calculate macro-average precision, recall, and F1-Score
macroPrecision = mean(precisionArray);
macroRecall = mean(recallArray);
macroF1Score = mean(f1ScoreArray);

% Calculate micro-average precision, recall, and F1-Score
microPrecision = sum(TP_micro) / (sum(TP_micro) + sum(FP_micro));
microRecall = sum(TP_micro) / (sum(TP_micro) + sum(FN_micro));
microF1Score = 2 * (microPrecision * microRecall) / (microPrecision + microRecall);

% Display macro and micro averages
disp(['Macro-Average Precision: ' num2str(macroPrecision)]);
disp(['Macro-Average Recall: ' num2str(macroRecall)]);
disp(['Macro-Average F1-Score: ' num2str(macroF1Score)]);
disp(['Micro-Average Precision: ' num2str(microPrecision)]);
disp(['Micro-Average Recall: ' num2str(microRecall)]);
disp(['Micro-Average F1-Score: ' num2str(microF1Score)]);
